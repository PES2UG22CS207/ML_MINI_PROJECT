Used Car Price Prediction: Exploratory Analysis and Linear Regression

This repository contains the Jupyter Notebook pes2ug23cs046-pes2ug22cs207[1].ipynb, which demonstrates the process of predicting used car prices using a Linear Regression model on a large dataset of used car listings.

The notebook includes steps for data loading, exploration, cleaning, feature encoding, model training, and performance evaluation.

📊 Dataset

The project uses the 852K Used Car Listings dataset sourced from Kaggle.

⚠️ Note:
The training and testing datasets are too large (each exceeding hundreds of MBs) to be uploaded directly to GitHub.
To run the notebook successfully, please execute it on Kaggle Notebooks
, where the datasets are available publicly.

Dataset References
Purpose	Source File
Training Data	tc20171021.csv
Testing Data	true_car_listings.csv
🚀 Project Workflow
1. Data Ingestion and Cleaning

Loads training and test datasets from Kaggle.

Initial Review: Training data has 852,122 rows × 8 columns.

Cleaning: Standardized column names; no missing values in key columns (price, year, mileage, city, state, vin, make, model).

The vin column (Vehicle Identification Number) is excluded from modeling as it’s unique per car.

2. Exploratory Data Analysis (EDA) Summary
Feature	Type	Key Observation
price	Numerical (Target)	Ranges from 1,500 to 499,500
year	Numerical	Ranges from 1997 to 2018
mileage	Numerical	Maximum ~2.8M → potential outliers
city	Categorical	2,553 unique cities
state	Categorical	59 unique values (case variations)
make	Categorical	58 manufacturers
model	Categorical	2,736 unique models
3. Feature Preprocessing and Encoding

Target: price extracted as Y, non-numeric rows dropped.

Categorical Encoding: state, city, make, model encoded using OrdinalEncoder.

📝 Ordinal encoding assigns an integer rank to each category.

4. Linear Regression Model & Evaluation

Model: sklearn.linear_model.LinearRegression

Trained on the processed dataset.

Performance Metrics
Metric	Value	Interpretation
R² Score	0.206	Explains ~20.6% variance → basic linear relationship
RMSE	12,118.04	Average prediction error ≈ $12K
Feature Influence
Feature	Coefficient	Influence
year	+742.56	Strong positive effect (newer cars = higher price)
make	-66.41	Moderate negative
state	+22.88	Slight positive
mileage	-0.09	Negative — higher mileage lowers price
🔍 Next Steps

Outlier Handling: Remove or cap extreme mileage and price values.

Advanced Encoding: Use One-Hot Encoding for categorical variables.

Model Upgrade: Try Random Forest, Gradient Boosting, or XGBoost for non-linear modeling.

🧠 How to Run the Notebook

Since the dataset is large, run this notebook directly on Kaggle:

Visit Kaggle Datasets
 and search for “Used Car Listings” or “tc20171021.csv”.

Create a Kaggle Notebook.

Upload or link the dataset from Kaggle’s dataset section.

Upload this .ipynb notebook to your Kaggle workspace and run all cells.
